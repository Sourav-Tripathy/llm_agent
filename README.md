# llm_agent
This is a sample agent implementation with self hosted model through ollama where your code accesses it through a fastapi server.

The outputs can be improved upon proper structuring of prompts for the model and better models.
